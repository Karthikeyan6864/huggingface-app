{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7XEtQHXEqS4"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install transformers datasets\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Disable W&B Logging (uncomment to disable W&B completely)\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable W&B logging\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/GuviMentor88/Training-Datasets/refs/heads/main/twitter_training.csv\"\n",
        "column_names = [\"Tweet_Id\", \"Entity\", \"label\", \"text\"]\n",
        "\n",
        "\n",
        "df = pd.read_csv(url, names=column_names, header=None)\n",
        "\n",
        "# Preprocess the data\n",
        "df_cleaned = df.copy()\n",
        "label_mapping = {\"Positive\": 0, \"Neutral\": 1, \"Negative\": 2}\n",
        "\n",
        "# Use .loc to avoid SettingWithCopyWarning\n",
        "df_cleaned.loc[:, 'label'] = df_cleaned['label'].replace({'Irrelevant': 'Neutral'})\n",
        "df_cleaned.loc[:, 'label'] = df_cleaned['label'].map(label_mapping)\n",
        "\n",
        "# Text cleaning\n",
        "df_cleaned.loc[:, 'text'] = df_cleaned['text'].str.replace(r'http\\S+|www\\S+|pic\\.twitter\\.com/\\S+', '', regex=True)\n",
        "df_cleaned.loc[:, 'text'] = df_cleaned['text'].str.replace(r'@\\S+', '', regex=True)\n",
        "df_cleaned.loc[:, 'text'] = df_cleaned['text'].str.replace(r'#\\S+', '', regex=True)\n",
        "df_cleaned.loc[:, 'text'] = df_cleaned['text'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)\n",
        "df_cleaned.loc[:, 'text'] = df_cleaned['text'].str.lower().str.strip().replace(r'\\s+', ' ', regex=True)\n",
        "\n",
        "# Split the data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df_cleaned[\"text\"].tolist(), df_cleaned[\"label\"].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "train_texts = [str(text) for text in train_texts]\n",
        "val_texts = [str(text) for text in val_texts]\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Convert to Hugging Face datasets format\n",
        "train_dataset = Dataset.from_dict({\"text\": train_texts, \"label\": train_labels})\n",
        "val_dataset = Dataset.from_dict({\"text\": val_texts, \"label\": val_labels})\n",
        "\n",
        "# Tokenize the datasets\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "train_dataset = train_dataset.remove_columns([\"text\"])\n",
        "val_dataset = val_dataset.remove_columns([\"text\"])\n",
        "\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "train_dataset.set_format(\"torch\")\n",
        "val_dataset.set_format(\"torch\")\n",
        "\n",
        "# Load the model\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "\n",
        "# Define training arguments (with W&B run name customization or disabling)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=1,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    \n",
        ")\n",
        "\n",
        "# Define metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "# Train the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Save the model locally\n",
        "local_model_path = \"./model\"\n",
        "model.save_pretrained(local_model_path)\n",
        "tokenizer.save_pretrained(local_model_path)\n",
        "\n",
        "# Save the model to Google Drive\n",
        "drive_model_path = \"/content/drive/My Drive/distilbert_model\"\n",
        "os.makedirs(drive_model_path, exist_ok=True)\n",
        "\n",
        "!cp -r ./model/* \"{drive_model_path}\"\n",
        "print(f\"Model saved to Google Drive at {drive_model_path}\")\n",
        "\n",
        "\n",
        "from shutil import make_archive\n",
        "make_archive(\"distilbert_model\", 'zip', local_model_path)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"distilbert_model.zip\")\n",
        "\n",
        "print(\"Model and tokenizer saved successfully!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
